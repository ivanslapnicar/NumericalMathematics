### A Pluto.jl notebook ###
# v0.14.7

using Markdown
using InteractiveUtils

# ‚ïî‚ïê‚ï° 38d8c023-f8b4-423f-8fdb-22be688c9030
# On your computer, comment this cell
begin
	import Pkg
	Pkg.activate(mktempdir())
    Pkg.add([
        Pkg.PackageSpec(name="PlutoUI"),
		Pkg.PackageSpec(name="Statistics"),
		Pkg.PackageSpec(name="Clustering"),
		Pkg.PackageSpec(name="Plots")
    ])
end

# ‚ïî‚ïê‚ï° d0233ece-1f69-11eb-3da1-43b8ddc9fbe1
begin
	# Necessary packages
	using PlutoUI, LinearAlgebra, Random, Statistics, Clustering, Plots
	plotly()
end

# ‚ïî‚ïê‚ï° c7e41465-41ad-4f11-baec-bfaae9edbf59
TableOfContents(title="üìö Table of Contents", aside=true)

# ‚ïî‚ïê‚ï° 00d72ad7-ea80-4954-8c75-590d4e9ccdc4
md"""
# K-means Algorithm


Data clustering is one of the main mathematical applications variety of algorithms have been developed to tackle the problem. K-means is one of the basic algorithms for data clustering.

__Prerequisites__

The reader should be familiar with basic linear algebra. 
 
__Competences__

The reader should be able to recognise applications where K-means algorithm can be efficiently used and use it.

__Credits.__ The notebook was initially derived from M.Sc. Thesis of Ivanƒçica Miro≈°eviƒá. 
"""

# ‚ïî‚ïê‚ï° 235e1901-eed3-4934-9214-ffdd82cf21d6
md"""
## Definitions

__Data clustering problem__ is the following: partition the given set of $m$ objects of the same type into $k$ subsets according to some criterion. Additional request may be to find the optimal $k$.

__K-means clustering problem__ is the following: partition the set  $X=\{x_{1},x_{2},\cdots ,x_{m}\}$ , where $x_{i}\in\mathbb{R}^{n}$, into $k$ _clusters_ $\pi=\{C_{1},C_{2},...,C_{k}\}$ such that

$$
J(\pi)=\sum_{i=1}^{k}\sum_{x\in
C_{i}}\| x-c_{i}\|_{2}^{2} \to \min$$

over all possible partitions. Here $c_{i}=\displaystyle\frac{1}{|C_{i}|}\sum_{x\in C_{i}} x$ is the mean of points in $C_i$ and $|C_i|$ is the cardinality of $C_i$.
"""

# ‚ïî‚ïê‚ï° 26640b90-1f69-11eb-029e-4dc3a825f639
md"
## K-means clustering algorithm

1. __Initialization__: Choose initial set of $k$ means $\{c_1,\ldots,c_k\}$ (for example, by choosing randomly $k$ points from $X$).
2. __Assignment step__: Assign each point $x$ to one nearest mean $c_i$.
3. __Update step__: Compute the new means.
4. __Iteration__: Repeat Steps 2 and 3 until the assignment no longer changes.
"

# ‚ïî‚ïê‚ï° 48914840-1f69-11eb-30a9-3943767bc261
md"
## First Variation clustering algorithm

A __first variation__ of a partition $\pi=\{C_1,\ldots,C_k\}$ is 
a partition $\pi^{\prime}=\{C_{1}^{\prime},\cdots ,C_{k}^{\prime }\}$ 
obtained by moving a single point $x$ from a cluster  $C_{i}$ to a cluster $C_{j}$. Notice that $\pi$ is a first variation of itself.

A __next partition__ of the partition $\pi$ is a partition 
$\mathop{\mathrm{next}}(\pi)=\mathop{\mathrm{arg min}}\limits_{\pi^{\prime}} J(\pi^{\prime})$.

We have the following algorithm:

1. Choose initial partition $\pi$.
2. Compute $\mathop{\mathrm{next}}(\pi)$
3. If $J(\mathop{\mathrm{next}}(\pi))<J(\pi)$, set $\pi=\mathop{\mathrm{next}}(\pi)$ and go to Step 2
4. Stop.
"

# ‚ïî‚ïê‚ï° 2b76953c-edf2-41cf-9d75-7efb2d6a58f1
md"""
## Facts

1. The k-means clustering problem is NP-hard.

2. In the k-means algorithm, $J(\pi)$ decreases in every iteration.

3. K-means algorithm can converge to a local minimum.

4. Each iteration of the k-means algorithm requires $O(mnk)$ operations.

4. K-means algorithm is implemented in the function `kmeans()` in the package [Clustering.jl](https://github.com/JuliaStats/Clustering.jl).

5.  $J(\pi)=\mathop{\mathrm{trace}}(S_W)$, where

$$
S_{W}=\sum\limits_{i=1}^k\sum\limits_{x\in C_{i}}
(x-c_i)(x-c_{i})^{T}
=\sum_{i=1}^k\frac{1}{2|C_{i}|}\sum_{x\in C_{i}}\sum_{y \in C_{i}}
(x-y)(x-y)^{T}.$$

Let $c$ denote the mean of $X$. Then $S_W=S_T-S_B$, where

$$
\begin{aligned}
S_{T}&=\sum_{x\in X}(x-c)(x-c)^{T} = 
\frac{1}{2m}\sum_{i=1}^m\sum_{j=1}^m
(x_{i}-x_{j})(x_{i}-x_{j})^{T}, \\
S_{B}&=\sum_{i=1}^k|C_{i}|(c_{i}-c)(c_{i}-c)^{T} =
\frac{1}{2m}\sum_{i=1}^k\sum_{j=1}^k|C_{i}||C_{j}|
(c_{i}-c_{j})(c_{i}-c_{j})^{T}.
\end{aligned}$$

6. In order to try to avoid convergence to local minima, the k-means algorithm can be enhanced with first variation by adding the following steps:
    1. Compute $\mathop{\mathrm{next}}(\pi)$. 
    2. If $J(\mathop{\mathrm{next}}(\pi))<J(\pi)$, set $\pi=\mathop{\mathrm{next}}(\pi)$ and go to Step 2.
     
"""

# ‚ïî‚ïê‚ï° 86d55833-ee5a-4f73-b0bf-e8b6e6d65617
function myKmeans(X::Vector{T}, k::Int) where T
    # X is Array of Arrays
    m,n=length(X),length(X[1])
    C=Vector{Int}(undef,m)
    # Choose random k means among X
    c=X[randperm(m)[1:k]]
    # This is just to start the while loop
    cnew=copy(c)
    cnew[1]=cnew[1].+(1.0,1.0)
    # Loop
    iterations=0
    while cnew!=c
        iterations+=1
        cnew=copy(c)
        # Assignment
        for i=1:m
            C[i]=findmin([norm(X[i].-c[j]) for j=1:k])[2]
        end
        # Update
        for j=1:k
          c[j]=(mean([x[1] for x in X[C.==j]]),mean([x[2] for x in X[C.==j]]))
        end
    end
    C,c,iterations
end

# ‚ïî‚ïê‚ï° e15fa4cf-930a-4065-b839-c2ec944afcfa
md"""

## Examples

### Random clusters

We generate $k$ random clusters around points with integer coordinates.
"""

# ‚ïî‚ïê‚ï° bacdc83d-1acd-46df-8d06-3e0279978a33
begin
	# Generate points as Tuple()
	k=5
	Random.seed!(1235)
	centers= [Tuple(rand(-5:5,2)) for i=1:k]
	# Number of points in cluster
	sizes=rand(10:50,k)
	csizes=cumsum(sizes)
	# X is array of arrays
	X=Vector{Tuple{Float64,Float64}}(undef,sum(sizes))
	X[1:csizes[1]]=[centers[1].+Tuple((rand(2).-0.5)/2) for i=1:sizes[1]]
	for j=2:k
		X[csizes[j-1]+1:csizes[j]]=[centers[j].+Tuple((rand(2).-0.5)/2) for i=1:sizes[j]]
	end
	centers, sizes, X
end

# ‚ïî‚ïê‚ï° fceab134-0921-459b-8d91-952a44abb07b
begin
	# Plot
	scatter(X,label="Points",title="Points")
	scatter!(centers,markershape = :hexagon, ms = 6,label="Centers")
end

# ‚ïî‚ïê‚ï° f559b870-608a-426e-a23b-b66ef6d4b47f
# Plot the solution
function plotKmeansresult(C::Vector,c::Vector,X::Vector)
    scatter()
    # Clusters
    for j=1:k
        scatter!(X[findall(C.==j)],label="Cluster $j")
    end
    # Means
    scatter!(c,markershape=:hexagon,ms=6,color=:red,label="Centers")
	plot!(title="Computed clusters")
end

# ‚ïî‚ïê‚ï° 310a4957-cc58-424b-845c-3ebaf7db77f3
md"""
__What happens?__

We see that the algorithm, although simple, for this example 
converges to a local minimum.

Let us try the function `kmeans()` from the package `Clustering.jl`.
The inputs to `kmeans()` are:

* a matrix whose columns are points, 
* number of clusters we are looking for, and, 
* optionally, the method to compute initial means. 

If we choose `init=:rand`, the results are similar. If we choose
`init=:kmpp`, which is the default, the results are better, but convergence to a local minimum is still possible.

__Run the clustering several times!__

```
seeding_algorithm(s::Symbol) = 
    s == :rand ? RandSeedAlg() :
    s == :kmpp ? KmppAlg() :
    s == :kmcen ? KmCentralityAlg() :
    error("Unknown seeding algorithm $s")
```
"""

# ‚ïî‚ïê‚ï° f9af8a75-48fd-4e82-b43e-61867bead6f9
methods(kmeans)

# ‚ïî‚ïê‚ï° 7e869699-8eb1-4e38-905d-a9448d037841
begin
	X‚Çò=transpose([[x[1] for x in X] [x[2] for x in X]])
	output=kmeans(Matrix(X‚Çò),k,init=:kmpp)
end

# ‚ïî‚ïê‚ï° 4f8152ee-ce86-4df6-81d2-be2df33df980
fieldnames(KmeansResult)

# ‚ïî‚ïê‚ï° 95fb2e66-e811-47f4-a772-8270d0c5d83b
output.centers

# ‚ïî‚ïê‚ï° 26b7bac0-1f72-11eb-16fc-c1d74920c462
output.assignments

# ‚ïî‚ïê‚ï° 7bb0e9ce-72dd-4623-ab3d-a6d18fb9dd15
# We need to modify the plotting function
function plotKmeansresult(out::KmeansResult,X::AbstractArray)
    k=size(out.centers,2)
    scatter(aspect_ratio=1)
    # Clusters
    for j=1:k
        scatter!(X[1,findall(out.assignments.==j)], X[2,findall(out.assignments.==j)], label="Cluster $j")
    end
    # Means
    scatter!(out.centers[1,:], out.centers[2,:], markershape=:hexagon,ms=6,color=:red,label="Centers") 
end

# ‚ïî‚ïê‚ï° 8e5b89ce-e6b5-4473-bd98-a2285331418c
begin
	C,c,iterations=myKmeans(X,k)
	plotKmeansresult(C,c,X)
end

# ‚ïî‚ïê‚ï° 406bb1b2-a8bb-4cd2-95c3-3975f99e3f32
begin
	out=kmeans(X‚Çò,k,init=:kmpp)
	plotKmeansresult(out,X‚Çò)
end

# ‚ïî‚ïê‚ï° 2445e75a-77f9-4c27-b400-44d8b49fc03b
md"""
### Concentric rings

The k-means algorithm works well if clusters can be separated by hyperplanes. In this example it is not the case.
"""

# ‚ïî‚ïê‚ï° 21bdc615-76e8-46cc-ac86-310d28f6cd25
begin
	# Number of rings, try also k=3
	k‚ÇÅ=2
	# Center
	Random.seed!(5361)
	center=[rand(-5:5);rand(-5:5)]
	# Radii
	radii=randperm(10)[1:k‚ÇÅ]
	# Number of points in circles
	sizes‚ÇÅ=rand(1000:2000,k‚ÇÅ)
	center,radii,sizes‚ÇÅ
end

# ‚ïî‚ïê‚ï° 9a4060b7-ba5a-401f-90f2-b9780ed93ddb
begin
	# Generate points
	X‚ÇÅ=Array{Float64}(undef,2,sum(sizes‚ÇÅ))
	csizes‚ÇÅ=cumsum(sizes‚ÇÅ)
	# Random angles
	œï=2*œÄ*rand(sum(sizes‚ÇÅ))
	for i=1:csizes‚ÇÅ[1]
		X‚ÇÅ[:,i]=center+radii[1]*[cos(œï[i]);sin(œï[i])] + (rand(2).-0.5)/50
	end
	for j=2:k‚ÇÅ
		for i=csizes‚ÇÅ[j-1]+1:csizes‚ÇÅ[j]
			X‚ÇÅ[:,i]=center+radii[j]*[cos(œï[i]);sin(œï[i])] + (rand(2).-0.5)/50
		end
	end
	scatter(X‚ÇÅ[1,:],X‚ÇÅ[2,:],title="Concentric Rings", aspect_ratio=1,label="Points")
end

# ‚ïî‚ïê‚ï° 0be24dd5-f3b0-4daa-aad8-5dbcdfca9313
begin
	out‚ÇÅ=kmeans(X‚ÇÅ,k‚ÇÅ,init=:rand)
	plotKmeansresult(out‚ÇÅ,X‚ÇÅ)
end

# ‚ïî‚ïê‚ï° 1637ce15-f55a-4d66-8a00-45a555bf5d61


# ‚ïî‚ïê‚ï° Cell order:
# ‚ï†‚ïê38d8c023-f8b4-423f-8fdb-22be688c9030
# ‚ï†‚ïêd0233ece-1f69-11eb-3da1-43b8ddc9fbe1
# ‚ï†‚ïêc7e41465-41ad-4f11-baec-bfaae9edbf59
# ‚ïü‚îÄ00d72ad7-ea80-4954-8c75-590d4e9ccdc4
# ‚ïü‚îÄ235e1901-eed3-4934-9214-ffdd82cf21d6
# ‚ïü‚îÄ26640b90-1f69-11eb-029e-4dc3a825f639
# ‚ïü‚îÄ48914840-1f69-11eb-30a9-3943767bc261
# ‚ïü‚îÄ2b76953c-edf2-41cf-9d75-7efb2d6a58f1
# ‚ï†‚ïê86d55833-ee5a-4f73-b0bf-e8b6e6d65617
# ‚ïü‚îÄe15fa4cf-930a-4065-b839-c2ec944afcfa
# ‚ï†‚ïêbacdc83d-1acd-46df-8d06-3e0279978a33
# ‚ï†‚ïêfceab134-0921-459b-8d91-952a44abb07b
# ‚ï†‚ïêf559b870-608a-426e-a23b-b66ef6d4b47f
# ‚ï†‚ïê8e5b89ce-e6b5-4473-bd98-a2285331418c
# ‚ïü‚îÄ310a4957-cc58-424b-845c-3ebaf7db77f3
# ‚ï†‚ïêf9af8a75-48fd-4e82-b43e-61867bead6f9
# ‚ï†‚ïê7e869699-8eb1-4e38-905d-a9448d037841
# ‚ï†‚ïê4f8152ee-ce86-4df6-81d2-be2df33df980
# ‚ï†‚ïê95fb2e66-e811-47f4-a772-8270d0c5d83b
# ‚ï†‚ïê26b7bac0-1f72-11eb-16fc-c1d74920c462
# ‚ï†‚ïê7bb0e9ce-72dd-4623-ab3d-a6d18fb9dd15
# ‚ï†‚ïê406bb1b2-a8bb-4cd2-95c3-3975f99e3f32
# ‚ïü‚îÄ2445e75a-77f9-4c27-b400-44d8b49fc03b
# ‚ï†‚ïê21bdc615-76e8-46cc-ac86-310d28f6cd25
# ‚ï†‚ïê9a4060b7-ba5a-401f-90f2-b9780ed93ddb
# ‚ï†‚ïê0be24dd5-f3b0-4daa-aad8-5dbcdfca9313
# ‚ï†‚ïê1637ce15-f55a-4d66-8a00-45a555bf5d61
