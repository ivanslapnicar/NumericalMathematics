### A Pluto.jl notebook ###
# v0.14.7

using Markdown
using InteractiveUtils

# ‚ïî‚ïê‚ï° 271cba4c-ed08-497e-8142-c99aea337cd8
# On your computer, comment this cell
begin
	import Pkg
	Pkg.activate(mktempdir())
    Pkg.add([
        Pkg.PackageSpec(name="PlutoUI"),
		Pkg.PackageSpec(name="ForwardDiff"),
		Pkg.PackageSpec(name="Plots"),
		Pkg.PackageSpec(name="NLsolve"),
		Pkg.PackageSpec(name="Optim")
    ])
end

# ‚ïî‚ïê‚ï° 57657193-e72f-432f-965b-13f36c104874
begin
	using PlutoUI, ForwardDiff, Plots, LinearAlgebra, NLsolve, Optim
	plotly()
end

# ‚ïî‚ïê‚ï° 63690bb6-5749-429d-8c39-dd9c727bc938
TableOfContents(title="üìö Table of Contents", aside=true)

# ‚ïî‚ïê‚ï° 94da0d5c-6dfe-487a-9711-9d101f20e97a
md"""
# Systems of Nonlinear Equations


__Problem.__ Let us find a solution $\xi=(\xi_1,\xi_2,\ldots,\xi_n)$ of the system of $n$ equations 

$$\begin{aligned}
f_1(x)&=0,\cr
f_2(x)&=0,\cr
&\vdots \cr
f_n(x)&=0,
\end{aligned}$$

and $n$ unknowns $x=(x_1,x_2,\ldots,x_n)$. Denoting $f=(f_1,f_2,\ldots,f_n)^T$, we can write this system as

$$
f(x)=0.$$

We shall describe the __Newton's method__ and three __quasi-Newton__ methods:

2. __Broyden's__ method,
3. __Davidon-Fletcher-Powell__ method, and 
3. __Broyden-Fletcher-Goldfarb-Schano__ method.

Given starting approximation $x^{(0)}$, all methods generate sequence of points $x^{(n)}$ which, under certain condditions, converges towards the solution $\xi$. 

__Remark.__ For detailed description of the methods and examples see [Numerical Methods for Unconstrained Optimization and Nonlinear Equations](https://epubs.siam.org/doi/book/10.1137/1.9781611971200?mobileUi=0) or [Numeriƒçka matematika, section 4.4](http://www.mathos.unios.hr/pim/Materijali/Num.pdf).
"""

# ‚ïî‚ïê‚ï° 125c9230-cc68-493f-92c5-8a83a78b5863
md"""
# Newton's method

__Jacobian__ or __Jacobi matrix__ of functions $f$ in the point $x$ is the matrix of first partial derivatives

$$
J(f,x)=\begin{pmatrix} \displaystyle\frac{\partial f_1(x)}{\partial x_1} & \displaystyle\frac{\partial f_1(x)}{\partial x_2} & \cdots &
\displaystyle\frac{\partial f_1(x)}{\partial x_n} \\
\displaystyle\frac{\partial f_2(x)}{\partial x_1} & \displaystyle\frac{\partial f_2(x)}{\partial x_2} & \cdots &
\displaystyle\frac{\partial f_2(x)}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\displaystyle\frac{\partial f_n(x)}{\partial x_1} & \displaystyle\frac{\partial f_n(x)}{\partial x_2} & \cdots &
\displaystyle\frac{\partial f_n(x)}{\partial x_n} 
\end{pmatrix}.$$

Given starting approximation $x^{(0)}$, we compute the sequence of points

$$
x^{(k+1)}=x^{(k)}-s^{(k)}, \quad k=0,1,2,\ldots,$$

where $s^{(k)}$ is the solution of the system of linear equations

$$
J\big(f,x^{(k)}\big)\cdot s=f\big(x^{(k)}\big).$$

Jacobians are computed using the package [`ForwardDiff.jl`](http://www.juliadiff.org/ForwardDiff.jl/perf_diff.html#derivatives). Plotting is done using the package `Plots.jl`.
"""

# ‚ïî‚ïê‚ï° 77095a05-eb64-47e2-a2e0-5053a4fbc837
function Newton(f::Function,J::Function,x::Vector{T},œµ::Float64=1e-10) where T
    iter=0
    s=ones(T,size(x))
    Œæ=x
    while norm(s)>œµ && iter<100
        s=J(x)\f(x)
        Œæ=x-s
        iter+=1
        x=Œæ
    end
    Œæ,iter
end

# ‚ïî‚ïê‚ï° ed8971fd-1420-41af-b736-e052439a2c65
md"""
## Examples

### $2\times 2$ nonlinear system

The solutions of the system

$$\begin{aligned}
2(x+y)^2+(x-y)^2-8&=0\\
5x^2+(y-3)^2-9&=0
\end{aligned}$$

are the points $T_1=(1,1)$ and $T_2\approx(-1.18,1.59)$.
"""

# ‚ïî‚ïê‚ï° 96eb13a8-ca15-447b-a252-76d481b30912
# Vector function
f‚ÇÅ(x)=[2(x[1]+x[2])^2+(x[1]-x[2])^2-8,5*x[1]^2+(x[2]-3)^2-9]

# ‚ïî‚ïê‚ï° 7366924f-29e6-46a6-a5f1-e1940daa15dd
f‚ÇÅ((1.0,2))

# ‚ïî‚ïê‚ï° d7930d33-99f3-4551-8258-500bbde17aff
md"""
Let us plot the functions and the contours in order to approximately locate the zeros:
"""

# ‚ïî‚ïê‚ï° 46060676-7192-4e7d-9038-9492524c1485
begin
	# Number of points
	m=101
	X=range(-2,stop=3,length=m)
	Y=range(-2,stop=3,length=m)
	# First applicate
	surface(X,Y,(x,y)->f‚ÇÅ([x,y])[1],xlabel="x",ylabel="y",colorbar=false)
	# Second applicate
	surface!(X,Y,(x,y)->f‚ÇÅ([x,y])[2],seriescolor=:blues)
end

# ‚ïî‚ïê‚ï° 0234d953-b83d-48a1-9c32-a0e2b8bec20a
begin
	# Locate the solutions using contour plot
	contour(X,Y,(x,y)->f‚ÇÅ([x,y])[1],contour_labels=true)
	contour!(X,Y,(x,y)->f‚ÇÅ([x,y])[2],contour_labels=true)
end

# ‚ïî‚ïê‚ï° 7d4c9549-1233-4e03-ace4-1a1c87147036
# Clearer picture
contour!(clims=(0,0.01),xlabel="x",ylabel="y",colorbar=:none)

# ‚ïî‚ïê‚ï° e2b8b699-39e0-408a-8165-2f606ea25740
md"""
We see that the approximate zeros are $T_1=(-1.2,1.5)$ i $T_2=(1,1)$. Moreover, $T_2$ is exactly equal $(1,1)$ (one iteration in the third computation). Furthermore, the method does not always converge (fourth computation).   
"""

# ‚ïî‚ïê‚ï° 08505d4c-b799-4833-af0d-4efea3f925c1
J‚ÇÅ(x)=ForwardDiff.jacobian(f‚ÇÅ,x)

# ‚ïî‚ïê‚ï° a5631cd4-c6b7-4a7b-9411-adf83f7c15b3
# For example
J‚ÇÅ([1.0,2])

# ‚ïî‚ïê‚ï° 68d9ec70-cf1d-4260-8a66-5cce42c42eea
Newton(f‚ÇÅ,J‚ÇÅ,[-1.0,0.0]), Newton(f‚ÇÅ,J‚ÇÅ,[0.5,1.1]), 
Newton(f‚ÇÅ,J‚ÇÅ,[1.0,1.0]), Newton(f‚ÇÅ,J‚ÇÅ,[0.0,0.0])

# ‚ïî‚ïê‚ï° 3cfa6d61-75f0-4897-97d1-b94c8de2458e
md"""
### $2\times 2$ nonlinear system

The solutions of the system

$$\begin{aligned}
x_1^2-x_2^2-2&=0\\
e^{x_1-1}+x_2^3-2&=0
\end{aligned}$$

are the points $T_1=(1,1)$ and $T_2\approx (-0.71,1.22)$.
"""

# ‚ïî‚ïê‚ï° 2a1b5d9b-672f-4e00-8620-daec0196d6b7
begin
	f‚ÇÇ(x)=[x[1]^2+x[2]^2-2,exp(x[1]-1)+x[2]^3-2]
	contour(X,Y,(x,y)->f‚ÇÅ([x,y])[1],contour_labels=true)
	contour!(X,Y,(x,y)->f‚ÇÅ([x,y])[2],contour_labels=true)
	contour!(clims=(0,0.01),xlabel="x",ylabel="y",colorbar=:none)
end

# ‚ïî‚ïê‚ï° 6c8b54c2-d4ae-4ada-b0d9-cd83eafa9cc1
begin
	J‚ÇÇ(x)=ForwardDiff.jacobian(f‚ÇÇ,x)
	Newton(f‚ÇÇ,J‚ÇÇ,[-1.0,1]), Newton(f‚ÇÇ,J‚ÇÇ,[0.8,1.2])
end

# ‚ïî‚ïê‚ï° 2f2a6ae5-755d-4f65-9c16-4db2e1aa48f4
md"""
### $3\times 3$ nonlinear system

Solve $f(x)=0$, where

$$
f(x)=\begin{bmatrix}x_1 \\ x_2^2-x_2 \\ e^{x_3}-1 \end{bmatrix}.$$

The exact solutions are $T_1=(0,0,0)$ and $T_2=(0,-1,0)$. We shall compute the solutions using several starting points.
"""

# ‚ïî‚ïê‚ï° c6f98f59-312d-498d-9854-4c78c969011f
begin
	f‚ÇÉ(x)=[x[1],x[2]^2+x[2],exp(x[3])-1]
	J‚ÇÉ(x)=ForwardDiff.jacobian(f‚ÇÉ,x)
end

# ‚ïî‚ïê‚ï° ee2d1d89-d399-4d1f-8d08-c99e372d5537
Newton(f‚ÇÉ,J‚ÇÉ,[-1.0,1.0,0.0]),Newton(f‚ÇÉ,J‚ÇÉ,[1.0,1,1]),
Newton(f‚ÇÉ,J‚ÇÉ,[-1.0,1,-10]),Newton(f‚ÇÉ,J‚ÇÉ,[0.5,-1.5,0])

# ‚ïî‚ïê‚ï° e195e6ad-f3ea-4a74-a91a-d16d0384a054
md"""
### Rosenbrock parabolic valley

Given is the function 

$$
f(x)=100\,(x_2-x_1)^2+(1-x_1)^2.$$

We want to find possible extremal points, thet is, we want to solve the equation

$$
\mathop{\mathrm{grad}} f(x)=0.$$
"""

# ‚ïî‚ïê‚ï° 6e1b5ed8-3d23-4aad-b140-1567c8dc7336
f‚ÇÑ(x)=100(x[2]-x[1]^2)^2+(1-x[1])^2

# ‚ïî‚ïê‚ï° d9704e21-9e89-4b5d-bb4e-7dd24d6b506a
# Plot the function using X and Y from Example 1
surface(X,Y,(x,y)->f‚ÇÑ([x,y]), seriescolor=:blues, xlabel="x", ylabel="y")

# ‚ïî‚ïê‚ï° 008c4d22-4c2f-4087-8be7-47599600de47
begin
	# This function is very demanding w.r.t. finding extremal points
	g‚ÇÑ(x)=ForwardDiff.gradient(f‚ÇÑ,collect(x))
	contour(X,Y,(x,y)->g‚ÇÑ([x,y])[1],contour_labels=true)
	contour!(X,Y,(x,y)->g‚ÇÑ([x,y])[2],contour_labels=true)
	contour!(clims=(-0.5,0.5),xlabel="x",ylabel="y",colorbar=:none)
end

# ‚ïî‚ïê‚ï° ac8f76d2-9801-4f64-8add-278c8ada3135
md"""
By observing contours, we conclude that this example is numerically demanding, while it is easy to see analytically that the only zero is $T=(1,1)$.

Here the vector function is given as the gradient of the scalar function $f$, so the Jacobi matrix is computed using function `FowardDiff.hessian()` which approximates the matrix of second partial derivatives of the scalar function $f$.  
"""

# ‚ïî‚ïê‚ï° 34576988-89ce-431b-9e1c-78b1a98e002e
Newton(g‚ÇÑ,x->ForwardDiff.hessian(f‚ÇÑ,x),[-1.0,2.0])

# ‚ïî‚ïê‚ï° 9625fb8d-5e0e-4e07-bc49-b4cb9ab259ef
md"""
### Least-squares fitting of sum of exponentials

Let

$$
f(x)=\sum_{i=1}^{11} \bigg(x_3 \cdot \exp\bigg(-\frac{(t_i-x_1)^2}{x_2}\bigg)-y_i\bigg)^2,$$

where the pairs  $(t_i,y_i)$ are given by the table:

$$
\begin{array}{c|c|c|c|c|c|c|c|c|c|c|c}
 i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\ \hline
t_i & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 &10 \\
 y_i & 0.001 & .01 & .04 & .12 & .21 & .25 & .21 & .12 & .04 & .01 & .001 
\end{array}$$

We want to solve the equation

$$
\mathop{\mathrm{grad}} f(x)=0.$$

Unlike previous examples, where the condition number is $\kappa(J)=O(10)$ in the first three examples and $\kappa(J)=O(1000)$ in the previous example, here $\kappa(J)>O(10^6)$ so the method is inaccurate and does not converge towards the exact solution $x=(4.93,2.62,0.28)$.
"""

# ‚ïî‚ïê‚ï° 33ca0d51-92fe-4313-ba67-7792af17b11e
begin
	t=collect(0:10)
	y=[0.001,0.01,0.04,0.12,0.21,0.25,0.21,0.12,0.04,0.01,0.001]
	f‚ÇÖ(x)=sum([( x[3]*exp(-((t[i]-x[1])^2/x[2]))-y[1])^2 for i=1:11])
end

# ‚ïî‚ïê‚ï° e5d2cee7-c2d4-45a5-901c-206be7d2ff58
begin
	# Starting point is very near the solution
	x‚ÇÄ=[4.9,2.63,0.28]
	f‚ÇÖ(x‚ÇÄ)
	g‚ÇÖ(x)=ForwardDiff.gradient(f‚ÇÖ,x)
	J‚ÇÖ(x)=ForwardDiff.hessian(f‚ÇÖ,x)
	f‚ÇÖ(x‚ÇÄ), g‚ÇÖ(x‚ÇÄ), cond(J‚ÇÖ(x‚ÇÄ))
end

# ‚ïî‚ïê‚ï° ab87421c-e498-44ff-af23-13ea14a0c5d3
x‚ÇÖ,iter‚ÇÖ=Newton(g‚ÇÖ,J‚ÇÖ,x‚ÇÄ,1e-8)

# ‚ïî‚ïê‚ï° d9df727c-a96f-4af4-ab5a-60715662adce
g‚ÇÖ(x‚ÇÖ)

# ‚ïî‚ïê‚ï° 82fffd1c-9ec0-459c-b033-d926641268dc
Newton(g‚ÇÖ,J‚ÇÖ,[4.9,2.62,0.28],1e-8)

# ‚ïî‚ïê‚ï° e823348a-1ea3-45b7-b1ad-d51096f224dc
md"""
# Broyden's method

Given starting approximation $x_0$ and matrix $B_0$, for each $k=0,1,2,\ldots$, we compute:

$$\begin{aligned}
B_k \cdot s_k & = -f(x_k) \quad \textrm{(sustav)}\\
x_{k+1}&=x_{k}+s_k\\
y_k&=f(x_{k+1})-f(x_{k})\\
B_{k+1}&=B_k+\frac{(y_k-B_ks_k)s_k^T}{s_k\cdot s_k}
\end{aligned}$$

In this manner, we avoid the computation of the Jacobi matrix in each step. We can take $B_0=J(x_0)$, but also some other matrix.
"""

# ‚ïî‚ïê‚ï° 73ad34ac-e4fc-4e35-9d96-25969b37a4d6
function Broyden(f::Function,B::Matrix,x::Vector{T},œµ::Float64=1e-10) where T
    iter=0
    s=ones(T,length(x))
    Œæ=x
    while norm(s)>œµ && iter<100
        s=-(B\f(x))
        Œæ=x+s
        y=f(Œæ)-f(x)
        B=B+(y-B*s)*(s/(s‚ãÖs))'
        x=Œæ
        iter+=1
    end
    Œæ,iter
end

# ‚ïî‚ïê‚ï° 19dc36a5-6d13-4d61-a9b4-863e7fbe60fe
md"
## Examples
"

# ‚ïî‚ïê‚ï° 6059b96d-8ba0-475c-9929-8cc613355562
# Example 1
Broyden(f‚ÇÅ,J‚ÇÅ([-1.0,0.0]),[-1.0,0.0]), 
Broyden(f‚ÇÅ,J‚ÇÅ([1.0,1.5]),[1.0,1.5])

# ‚ïî‚ïê‚ï° 64d397ac-ed6f-426c-9139-af21222723da
begin
	# Explain behaviour the method when we set B‚ÇÄ=I
	eye(n)=Matrix{Float64}(I,n,n)
	Broyden(f‚ÇÅ,eye(2),[-1.0,0.0]), Broyden(f‚ÇÅ,eye(2),[1.0,1.5]),
	Broyden(f‚ÇÅ,eye(2),[-1,1.5])
end

# ‚ïî‚ïê‚ï° ba09dd4f-b114-45bb-9b6a-41c2ed9dbec5
begin
	# Example 2
	x0=[-1.0,1]
	x1=[0.8,1.2]
	Broyden(f‚ÇÇ,J‚ÇÇ([-1.0,1]),[-1.0,1]), Broyden(f‚ÇÇ,J‚ÇÇ([0.8,1.2]),[0.8,1.2])
end

# ‚ïî‚ïê‚ï° 52506f6c-069a-4883-8160-1cf415e80c36
# Example 3
Broyden(f‚ÇÉ,J‚ÇÉ([-1.0,1,0]),[-1.0,1,0]), Broyden(f‚ÇÉ,J‚ÇÉ([0.5,-1.5,0]),[0.5,-1.5,0])

# ‚ïî‚ïê‚ï° c0fb2bfc-7221-428d-b13d-ebb483bd0c1c
# Example 4
Broyden(g‚ÇÑ,(x->ForwardDiff.hessian(f‚ÇÑ,x))([-1.0,2]),[-1.0,2]), # ali
Broyden(g‚ÇÑ,(x->ForwardDiff.hessian(f‚ÇÑ,x))([1,2.0]),[-1.0,2]),
Broyden(g‚ÇÑ,(x->ForwardDiff.hessian(f‚ÇÑ,x))([0.8,0.5]),[0.8,0.5])

# ‚ïî‚ïê‚ï° 1fe960aa-f12a-4d3e-994a-1a5d5c10f29e
begin
	# Example 5
	x‚ÇÜ,iter‚ÇÜ=Broyden(g‚ÇÖ,(x->ForwardDiff.hessian(f‚ÇÖ,x))([4.9,2.6,0.2]), [4.9,2.6,0.2])
end

# ‚ïî‚ïê‚ï° b4e246f8-72b9-4042-8160-a1eda2084e45
g‚ÇÖ(x‚ÇÜ)

# ‚ïî‚ïê‚ï° 11259db5-f754-47c7-9e92-600adf6896f8
md"""
# Davidon-Fletcher-Powell (DFP) method

DFP is an optimization method which finds extremal points of the function $F:\mathbb{R}^n \to \mathbb{R}$, in which case $f(x)=\mathop{\mathrm{grad}}F(x)$.

Given initial approximation $x_0$ and matrix $H_0$, for $k=0,1,2,\ldots$, we compute:

$$\begin{aligned}
s_k&=-H_k f(x_k)\\
\beta_k&=\mathop{\mathrm{arg\ min}}_\beta F(x_{k}+\beta s_k) \\
s_k&=\beta_k s_k\\
x_{k+1}&=x_{k}+s_k \\
y_k&=f(x_{k+1})-f(x_{k})\\
H_{k+1}&=H_k+ \frac{s_k s_k^T}{y_k\cdot s_k}-\frac{H_k y_k y_k^T H_k}{y_k\cdot (H_k y_k)}.
\end{aligned}$$

We can take $H_0=I$. Notice that the iteration step does not require solving a system linear equations. Instead, all updates are performed using $O(n^2)$ operations.

The one-dimensional minimzation along the line $x_{k}+\beta s_k$ is preformed by finding zeros of the directional derivative using bisection.
"""

# ‚ïî‚ïê‚ï° ff33e971-4871-4533-95ef-4fd9b14f32d1
function Bisection(f::Function,a::T,b::T,œµ::Float64=1e-10) where T
    fa=f(a)
    fb=f(b)
    x=T
    fx=T
    if fa*fb>zero(T)
        # return "Incorrect interval"
        if abs(fa)>abs(fb)
            return b,fb,0
        else
            return a,fa,0
        end
    end
    iter=0
    while b-a>œµ && iter<1000
        x=(b+a)/2.0
        fx=f(x)
        if fa*fx<zero(T)
            b=x
            fb=fx
        else
            a=x
            fa=fx
        end
        iter+=1
        # @show x,fx
    end
    x,fx,iter
end

# ‚ïî‚ïê‚ï° f29daf79-022a-4f7d-8616-6cac0e7ea9e8
function DFP(f::Function,H::Matrix,x::Vector{T},œµ::Float64=1e-10) where T
    iter=0
    s=ones(T,length(x))
    Œæ=x
    while norm(s)>œµ && iter<50
        s=-H*f(x)
        s0=s/norm(s)
        F(Œ∂)=f(x+Œ∂*s)‚ãÖs0
        Œ≤,fx,iterb=Bisection(F,0.0,1.0,10*eps())
        s*=Œ≤
        Œæ=x+s
        y=f(Œæ)-f(x)
        z=H*y
        H=H+(s/(y‚ãÖs))*s'-(z/(y‚ãÖz))*z'
        x=Œæ
        iter+=1
    end
    Œæ,iter
end

# ‚ïî‚ïê‚ï° 7f7672da-787f-4628-83aa-e9297f7da322
md"""
### Minimum of a function of $2$ variables

Let us find extremal points of the function 

$$
f(x,y)=(x+2y-7)^2+(2x+y-5)^2.$$

The function has minimum at $(1,3)$.
"""

# ‚ïî‚ïê‚ï° 8f4fc906-99a7-4e06-8620-83cf427da8b9
f‚ÇÜ(x) = (x[1] + 2*x[2]-7)^2 + (2*x[1] + x[2]-5)^2

# ‚ïî‚ïê‚ï° b687e934-69d9-4608-b73f-0f3676c210ec
f‚ÇÜ([1,2])

# ‚ïî‚ïê‚ï° 7ebf3fb9-85c7-4ea0-a18a-a4617375082d
g‚ÇÜ(x)=ForwardDiff.gradient(f‚ÇÜ,x)

# ‚ïî‚ïê‚ï° 1f6187a2-41a2-43e0-9cd3-322a069222aa
DFP(g‚ÇÜ,eye(2),[0.8,2.7],eps())

# ‚ïî‚ïê‚ï° d1e5bfe4-1ac9-4dbe-aafb-f81822a72d3a
# Example 4
DFP(g‚ÇÑ,eye(2),[0.9,1.1])

# ‚ïî‚ïê‚ï° 02ef6f4f-3259-447e-878a-b116a3d0eb4a
# Example 5
DFP(g‚ÇÖ,eye(3),[4.9,2.6,0.2])

# ‚ïî‚ïê‚ï° f7483843-9cc9-4799-ad56-1e74bd0a08e7
md"""
# Broyden-Fletcher-Goldfarb-Schano (BFGS) method

BFGS is an optimization method for finding extremal points of the function $F:\mathbb{R}^n \to \mathbb{R}$, in which case $f(x)=\mathop{\mathrm{grad}}F(x)$.

The method is similar to the DFP method, with somewhat better convergence properties.

Let $F:\mathbb{R}^n \to \mathbb{R}$, whose minmum we seek, and let $f(x)=\mathop{\mathrm{grad}} F(x)$.

Given initial approximation $x_0$ and matrix $H_0$, for $k=0,1,2,\ldots$, we compute:

$$\begin{aligned}
s_k&=-H_k f(x_k)\\
\beta_k&=\mathop{\mathrm{arg\ min}}F(x_{k}+\beta_k s_k) \\
s_k&=\beta_k s_k\\
x_{k+1}&=x_{k}+s_k \\
y_k&=f(x_{k+1})-f(x_{k})\\
H_{k+1}&=\bigg(I-\frac{s_k y_k^T}{y_k\cdot s_k}\bigg)H_k
\bigg( I-\frac{y_k s_k^T}{y_k\cdot s_k}\bigg)+\frac{s_k s_k^T}{y_k\cdot s_k}.
\end{aligned}$$

We can take $H_0=I$. Notice that the iteration step does not require solving a system linear equations. Instead, all updates are performed using $O(n^2)$ operations.

The one-dimensional minimzation along the line $x_{k}+\beta s_k$ is preformed by finding zeros of the directional derivative using bisection.
"""

# ‚ïî‚ïê‚ï° b985704f-8f07-43b6-baa9-6b3d4541de27
function BFGS(f::Function,H::Matrix,x::Vector{T},œµ::Float64=1e-10) where T
    iter=0
    s=ones(T,length(x))
    Œæ=x
    while norm(s)>œµ && iter<50
        s=-H*f(x)
        s0=s/norm(s)
        F(Œ∂)=f(x+Œ∂*s)‚ãÖs0
        Œ≤,fx,iterb=Bisection(F,0.0,1.0,10*eps())
        s*=Œ≤
        Œæ=x+s
        y=f(Œæ)-f(x)
        z=H*y
        Œ±=y‚ãÖs
        s1=s/Œ±
        H=H-s1*z'-z*s1'+s1*(y‚ãÖz)*s1'+s1*s'
        x=Œæ
        iter+=1
    end
    Œæ,iter
end

# ‚ïî‚ïê‚ï° 31a1b2aa-7b5b-46b0-82ec-f75111b72a0d
BFGS(g‚ÇÜ,eye(2),[0.8,2.7],eps())

# ‚ïî‚ïê‚ï° a0e4f6af-d4ab-4b04-9863-49de95eec947
# Example 4
BFGS(g‚ÇÑ,eye(2),[0.9,1.1])

# ‚ïî‚ïê‚ï° c20593d7-885f-4f36-984c-ecded0884ecc
# Example 5
BFGS(g‚ÇÖ,eye(3),[4.9,2.6,0.2])

# ‚ïî‚ïê‚ï° cef37fa4-df23-4267-bde4-c1fe6ddade23
md"""
# Julia packages

Previous programs are simple illustrative implementations of the mentioned algorithms.
Julia package [NLsolve.jl](https://github.com/JuliaNLSolvers/NLsolve.jl) is used for solving systems of linear equations and the package [Optim.jl](https://github.com/JuliaNLSolvers/Optim.jl) is used for nonlinear optimization.

## Examples
"""

# ‚ïî‚ïê‚ï° 92d27828-c97b-41d0-85e9-0d3b1738dfed
# Example 1
function f‚ÇÅ!(fvec,x)
    fvec[1] = 2(x[1]+x[2])^2+(x[1]-x[2])^2-8
    fvec[2] = 5*x[1]^2+(x[2]-3)^2-9
end

# ‚ïî‚ïê‚ï° 6055ba96-dd58-420b-854e-277b60a3c1e7
nlsolve(f‚ÇÅ!,[-1.0,0])

# ‚ïî‚ïê‚ï° ca176d00-70f9-42b4-a3f4-fd5d37c9f31a
nlsolve(f‚ÇÅ!,[0.5,1.1])

# ‚ïî‚ïê‚ï° a8080f5e-6797-482e-84ca-7ddfcd40264d
begin
	# Example 2
	function f‚ÇÇ!(fvec,x)
	    fvec[1] = x[1]^2+x[2]^2-2
	    fvec[2] = exp(x[1]-1)+x[2]^3-2
	end
	nlsolve(f‚ÇÇ!,[-1.0,1]), nlsolve(f‚ÇÇ!,[0.8,1.2])
end

# ‚ïî‚ïê‚ï° 2a644f10-2683-4ea8-bd25-95343abb7e48
begin
	# Example 3
	function f‚ÇÉ!(fvec,x)
	    fvec[1] = x[1]
	    fvec[2] = x[2]^2+x[2]
	    fvec[3] = exp(x[3])-1
	end
	nlsolve(f‚ÇÉ!,[-1.0,1.0,0.0]), nlsolve(f‚ÇÉ!,[1.0,1,1]),
	nlsolve(f‚ÇÉ!,[-1.0,1,-10]), nlsolve(f‚ÇÉ!,[0.5,-1.5,0])
end

# ‚ïî‚ïê‚ï° bc477197-2ac5-4798-a62b-62f89791fb94
# Example 4
optimize(f‚ÇÑ,[-1.0,2],Optim.BFGS())

# ‚ïî‚ïê‚ï° 90043461-3f6a-45e6-a661-57c8df3a2a93
# Example 5 - again there is no convergence
optimize(f‚ÇÖ,[4.9,2.6,0.2],Optim.BFGS())

# ‚ïî‚ïê‚ï° Cell order:
# ‚ï†‚ïê271cba4c-ed08-497e-8142-c99aea337cd8
# ‚ï†‚ïê57657193-e72f-432f-965b-13f36c104874
# ‚ï†‚ïê63690bb6-5749-429d-8c39-dd9c727bc938
# ‚ïü‚îÄ94da0d5c-6dfe-487a-9711-9d101f20e97a
# ‚ïü‚îÄ125c9230-cc68-493f-92c5-8a83a78b5863
# ‚ï†‚ïê77095a05-eb64-47e2-a2e0-5053a4fbc837
# ‚ïü‚îÄed8971fd-1420-41af-b736-e052439a2c65
# ‚ï†‚ïê96eb13a8-ca15-447b-a252-76d481b30912
# ‚ï†‚ïê7366924f-29e6-46a6-a5f1-e1940daa15dd
# ‚ïü‚îÄd7930d33-99f3-4551-8258-500bbde17aff
# ‚ï†‚ïê46060676-7192-4e7d-9038-9492524c1485
# ‚ï†‚ïê0234d953-b83d-48a1-9c32-a0e2b8bec20a
# ‚ï†‚ïê7d4c9549-1233-4e03-ace4-1a1c87147036
# ‚ïü‚îÄe2b8b699-39e0-408a-8165-2f606ea25740
# ‚ï†‚ïê08505d4c-b799-4833-af0d-4efea3f925c1
# ‚ï†‚ïêa5631cd4-c6b7-4a7b-9411-adf83f7c15b3
# ‚ï†‚ïê68d9ec70-cf1d-4260-8a66-5cce42c42eea
# ‚ïü‚îÄ3cfa6d61-75f0-4897-97d1-b94c8de2458e
# ‚ï†‚ïê2a1b5d9b-672f-4e00-8620-daec0196d6b7
# ‚ï†‚ïê6c8b54c2-d4ae-4ada-b0d9-cd83eafa9cc1
# ‚ïü‚îÄ2f2a6ae5-755d-4f65-9c16-4db2e1aa48f4
# ‚ï†‚ïêc6f98f59-312d-498d-9854-4c78c969011f
# ‚ï†‚ïêee2d1d89-d399-4d1f-8d08-c99e372d5537
# ‚ïü‚îÄe195e6ad-f3ea-4a74-a91a-d16d0384a054
# ‚ï†‚ïê6e1b5ed8-3d23-4aad-b140-1567c8dc7336
# ‚ï†‚ïêd9704e21-9e89-4b5d-bb4e-7dd24d6b506a
# ‚ï†‚ïê008c4d22-4c2f-4087-8be7-47599600de47
# ‚ïü‚îÄac8f76d2-9801-4f64-8add-278c8ada3135
# ‚ï†‚ïê34576988-89ce-431b-9e1c-78b1a98e002e
# ‚ïü‚îÄ9625fb8d-5e0e-4e07-bc49-b4cb9ab259ef
# ‚ï†‚ïê33ca0d51-92fe-4313-ba67-7792af17b11e
# ‚ï†‚ïêe5d2cee7-c2d4-45a5-901c-206be7d2ff58
# ‚ï†‚ïêab87421c-e498-44ff-af23-13ea14a0c5d3
# ‚ï†‚ïêd9df727c-a96f-4af4-ab5a-60715662adce
# ‚ï†‚ïê82fffd1c-9ec0-459c-b033-d926641268dc
# ‚ïü‚îÄe823348a-1ea3-45b7-b1ad-d51096f224dc
# ‚ï†‚ïê73ad34ac-e4fc-4e35-9d96-25969b37a4d6
# ‚ïü‚îÄ19dc36a5-6d13-4d61-a9b4-863e7fbe60fe
# ‚ï†‚ïê6059b96d-8ba0-475c-9929-8cc613355562
# ‚ï†‚ïê64d397ac-ed6f-426c-9139-af21222723da
# ‚ï†‚ïêba09dd4f-b114-45bb-9b6a-41c2ed9dbec5
# ‚ï†‚ïê52506f6c-069a-4883-8160-1cf415e80c36
# ‚ï†‚ïêc0fb2bfc-7221-428d-b13d-ebb483bd0c1c
# ‚ï†‚ïê1fe960aa-f12a-4d3e-994a-1a5d5c10f29e
# ‚ï†‚ïêb4e246f8-72b9-4042-8160-a1eda2084e45
# ‚ïü‚îÄ11259db5-f754-47c7-9e92-600adf6896f8
# ‚ï†‚ïêff33e971-4871-4533-95ef-4fd9b14f32d1
# ‚ï†‚ïêf29daf79-022a-4f7d-8616-6cac0e7ea9e8
# ‚ïü‚îÄ7f7672da-787f-4628-83aa-e9297f7da322
# ‚ï†‚ïê8f4fc906-99a7-4e06-8620-83cf427da8b9
# ‚ï†‚ïêb687e934-69d9-4608-b73f-0f3676c210ec
# ‚ï†‚ïê7ebf3fb9-85c7-4ea0-a18a-a4617375082d
# ‚ï†‚ïê1f6187a2-41a2-43e0-9cd3-322a069222aa
# ‚ï†‚ïêd1e5bfe4-1ac9-4dbe-aafb-f81822a72d3a
# ‚ï†‚ïê02ef6f4f-3259-447e-878a-b116a3d0eb4a
# ‚ïü‚îÄf7483843-9cc9-4799-ad56-1e74bd0a08e7
# ‚ï†‚ïêb985704f-8f07-43b6-baa9-6b3d4541de27
# ‚ï†‚ïê31a1b2aa-7b5b-46b0-82ec-f75111b72a0d
# ‚ï†‚ïêa0e4f6af-d4ab-4b04-9863-49de95eec947
# ‚ï†‚ïêc20593d7-885f-4f36-984c-ecded0884ecc
# ‚ïü‚îÄcef37fa4-df23-4267-bde4-c1fe6ddade23
# ‚ï†‚ïê92d27828-c97b-41d0-85e9-0d3b1738dfed
# ‚ï†‚ïê6055ba96-dd58-420b-854e-277b60a3c1e7
# ‚ï†‚ïêca176d00-70f9-42b4-a3f4-fd5d37c9f31a
# ‚ï†‚ïêa8080f5e-6797-482e-84ca-7ddfcd40264d
# ‚ï†‚ïê2a644f10-2683-4ea8-bd25-95343abb7e48
# ‚ï†‚ïêbc477197-2ac5-4798-a62b-62f89791fb94
# ‚ï†‚ïê90043461-3f6a-45e6-a661-57c8df3a2a93
